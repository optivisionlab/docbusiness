files:
  '__init__.py':
    changes:
      - line: 118
        content: 'from .layer.loss import FocalLoss'
      - line: 343
        content: "    'FocalLoss',"
  'functional/__init__.py':
    changes:
      - line: 102
        content: 'from .loss import focal_loss'
      - line: 256
        content: "    'focal_loss',"
  'functional/loss.py':
    changes:
      - line: 4113
        content: |
          def focal_loss(input, label, gamma=2.0, alpha=None, weight=None, ignore_index=-100, reduction='mean', soft_label=False, axis=-1, use_softmax=True, name=None):
              if reduction not in ['sum', 'mean', 'none']:
                  raise ValueError(
                      "The value of 'reduction' should be 'sum', 'mean', or 'none'. Received %s." % reduction
                  )
              if ignore_index > 0 and soft_label:
                  raise ValueError(
                      "When soft_label == True, ignore_index should be '-100'. Received %s." % ignore_index
                  )

              input_dims = len(list(input.shape))
              label_dims = len(list(label.shape))

              if input_dims - 1 == label_dims:
                  label = paddle.unsqueeze(label, axis=axis)

              if input_dims - 1 != label_dims and input_dims != label_dims:
                  raise ValueError(
                      'Expected input_dims - 1 == label_dims or input_dims == label_dims. Got input_dims{}, label_dims{}'.format(input_dims, label_dims)
                  )

              if not soft_label:
                  valid_label = paddle.cast(label != ignore_index, dtype=label.dtype) * label

              _, out = _C_ops.cross_entropy_with_softmax(input, label, soft_label, use_softmax, True, ignore_index, axis)

              pt = paddle.exp(-out)  # pt = exp(-cross_entropy)
              focal_weight = (1.0 - pt) ** gamma  # focal weight calculation
              out = focal_weight * out  # apply focal weight to the loss

              if weight is not None:
                  ignore_weight_mask = paddle.cast((label != ignore_index), out.dtype)

                  if ignore_weight_mask.ndim > 1 and ignore_weight_mask.shape[axis] == 1:
                      ignore_weight_mask = paddle.squeeze(ignore_weight_mask, axis)

                  if axis != -1 and axis != valid_label.ndim - 1:
                      temp_perm = (
                          list(range(axis % valid_label.ndim))
                          + list(range((axis % valid_label.ndim + 1), valid_label.ndim))
                          + [axis % valid_label.ndim]
                      )
                      weight_gather = _C_ops.gather_nd(weight, valid_label.transpose(temp_perm))
                  else:
                      weight_gather = _C_ops.gather_nd(weight, valid_label)

                  weight_gather = _C_ops.multiply(weight_gather, ignore_weight_mask)
                  input_shape = list(label.shape)
                  weight_gather_reshape = paddle.reshape(weight_gather, shape=input_shape)
                  out = paddle.cast(out, weight_gather_reshape.dtype)
                  out = _C_ops.multiply(out, weight_gather_reshape)

              if reduction == 'sum':
                  return _C_ops.sum(out, [], None, False)
              elif reduction == 'mean':
                  is_ignore = label == ignore_index
                  mask = ~is_ignore

                  if paddle.count_nonzero(is_ignore) > 0:
                      out_sum = _C_ops.sum(out, [], None, False)

                      if weight is None:
                          mask = paddle.cast(mask, dtype=out_sum.dtype)
                          count = _C_ops.sum(mask, [], None, False)
                          ret = out_sum / (count + (count == 0.0))
                      else:
                          mask = paddle.cast(mask, weight_gather_reshape.dtype)
                          weight_ignored = _C_ops.multiply(mask, weight_gather_reshape)
                          weight_sum = _C_ops.sum(weight_ignored, [], None, False)
                          ret = out_sum / (weight_sum + (weight_sum == 0.0))
                      return ret
                  elif weight is not None:
                      out_sum = _C_ops.sum(out, [], None, False)
                      total_weight = _C_ops.sum(weight_gather_reshape, [], None, False)
                      return out_sum / (total_weight + (total_weight == 0.0))
                  else:
                      return _C_ops.mean_all(out)
              else:
                  if input_dims - 1 == label_dims:
                      out = paddle.squeeze(out, axis=axis)
                  return out
  'layer/__init__.py':
    changes:
      - line: 90
        content: 'from .loss import FocalLoss'
  'layer/loss.py':
    changes:
      - line: 2142
        content: |
          class FocalLoss(Layer):
              def __init__(
                  self,
                  weight=None,
                  ignore_index=-100,
                  reduction='mean',
                  soft_label=False,
                  axis=-1,
                  use_softmax=True,
                  name=None,
                  alpha = None,
                  gamma = 2.0
              ):
                  super().__init__()
                  self.weight = weight
                  self.reduction = reduction
                  self.ignore_index = ignore_index
                  self.soft_label = soft_label
                  self.axis = axis
                  self.use_softmax = use_softmax
                  self.name = name
                  self.alpha = alpha
                  self.gamma = gamma

              def forward(self, input, label):
                  ret = paddle.nn.functional.focal_loss(
                      input,
                      label,
                      weight=self.weight,
                      ignore_index=self.ignore_index,
                      reduction=self.reduction,
                      soft_label=self.soft_label,
                      axis=self.axis,
                      use_softmax=self.use_softmax,
                      name=self.name,
                      alpha=self.alpha,
                      gamma= self.gamma
                  )

                  return ret